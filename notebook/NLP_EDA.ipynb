{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pickle\n",
    "from string import punctuation as punct\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create one big Doc object, pickle it for later use\n",
    "tweets = open('../data/tweets/tweet_text.txt', 'r').read()\n",
    "nlp.max_length = 2000000\n",
    "doc = nlp(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can comment out with statement after the pkl is written\n",
    "# with open('../data/tweets/tweets_nlp_doc.pkl', 'wb') as file:\n",
    "#     pickle.dump(doc, file)\n",
    "\n",
    "doc = pickle.load(open('../data/tweets/tweets_nlp_doc.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311160"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tokens in the entire tweet corpus\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('great', 2142), ('people', 1085), ('president', 874), ('country', 801), ('news', 758)]\n",
      "\n",
      "\n",
      "[('people', 1045), ('country', 528), ('time', 507), ('today', 494), ('years', 462)]\n"
     ]
    }
   ],
   "source": [
    "# all tokens that arent stop words or punctuations\n",
    "misc = ['\\n', '\\n  ', 'amp', ' ', '&amp;']\n",
    "\n",
    "words = [token.text.lower().strip().strip(punct) for token in doc if token.is_stop != True \n",
    "         and token.is_punct != True \n",
    "         and token.text not in misc]\n",
    "\n",
    "# noun tokens that arent stop words or punctuations\n",
    "nouns = [token.text.lower().strip().strip(punct) for token in doc if token.is_stop != True \n",
    "         and token.is_punct != True \n",
    "         and token.pos_ == \"NOUN\"]\n",
    "\n",
    "# five most common tokens\n",
    "word_freq = Counter(words)\n",
    "common_words = word_freq.most_common(5)\n",
    "print(common_words)\n",
    "print('\\n')\n",
    "\n",
    "# five most common noun tokens\n",
    "noun_freq = Counter(nouns)\n",
    "common_nouns = noun_freq.most_common(5)\n",
    "print(common_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great',\n",
       " 'people',\n",
       " 'president',\n",
       " 'country',\n",
       " 'news',\n",
       " 'democrats',\n",
       " 'thank',\n",
       " 'trump',\n",
       " 'big',\n",
       " 'fake',\n",
       " 'border',\n",
       " 'u.s',\n",
       " 'new',\n",
       " 'america',\n",
       " 'time',\n",
       " 'today',\n",
       " 'good',\n",
       " 'years',\n",
       " 'want',\n",
       " 'media',\n",
       " 'united',\n",
       " 'states',\n",
       " 'american',\n",
       " 'china',\n",
       " 'job',\n",
       " 'going',\n",
       " 'like',\n",
       " 'bad',\n",
       " 'house',\n",
       " 'vote',\n",
       " 'military',\n",
       " 'jobs',\n",
       " 'election',\n",
       " 'wall',\n",
       " 'crime',\n",
       " 'trade',\n",
       " 'state',\n",
       " 'dems',\n",
       " '',\n",
       " 'deal',\n",
       " 'way',\n",
       " 'security',\n",
       " 'russia',\n",
       " 'win',\n",
       " 'world',\n",
       " 'hunt',\n",
       " 'witch',\n",
       " 'collusion',\n",
       " 'said',\n",
       " 'day',\n",
       " 'working',\n",
       " 'republican',\n",
       " 'hard',\n",
       " 'republicans',\n",
       " 'north',\n",
       " 'history',\n",
       " 'year',\n",
       " 'strong',\n",
       " 'tax',\n",
       " 'work',\n",
       " 'look',\n",
       " 'fbi',\n",
       " 'long',\n",
       " 'total',\n",
       " 'obama',\n",
       " 'know',\n",
       " 'honor',\n",
       " 'congress',\n",
       " 'better',\n",
       " 'economy',\n",
       " 'korea',\n",
       " 'far',\n",
       " 'hillary',\n",
       " 'mueller',\n",
       " 'democrat',\n",
       " 'congratulations',\n",
       " 'campaign',\n",
       " 'need',\n",
       " 'mexico',\n",
       " 'foxnews',\n",
       " 'dollars',\n",
       " 'looking',\n",
       " 'national',\n",
       " 'things',\n",
       " 'getting',\n",
       " 'administration',\n",
       " 'help',\n",
       " 'report',\n",
       " 'got',\n",
       " 'best',\n",
       " 'coming',\n",
       " 'illegal',\n",
       " 'meeting',\n",
       " 'come',\n",
       " 'forward',\n",
       " 'countries',\n",
       " 'party',\n",
       " 'right',\n",
       " 'foxandfriends',\n",
       " 'immigration',\n",
       " 'ðŸ‡º',\n",
       " 'totally',\n",
       " 'clinton',\n",
       " 'senate',\n",
       " 'ðŸ‡¸',\n",
       " 'money',\n",
       " 'white',\n",
       " 'law',\n",
       " 'governor',\n",
       " 'crooked',\n",
       " 'story',\n",
       " 'wonderful',\n",
       " 'end',\n",
       " 'tariffs',\n",
       " 'high',\n",
       " 'stop',\n",
       " 'fact',\n",
       " 'bill',\n",
       " 'night',\n",
       " 'including',\n",
       " 'soon',\n",
       " 'record',\n",
       " 'making',\n",
       " 'usa',\n",
       " 'comey',\n",
       " 'let',\n",
       " 'justice',\n",
       " 'happy',\n",
       " 'borders',\n",
       " 'numbers',\n",
       " 'left',\n",
       " 'cnn',\n",
       " 'support',\n",
       " 'john',\n",
       " 'nation',\n",
       " 'love',\n",
       " 'washington',\n",
       " 'cuts',\n",
       " 'florida',\n",
       " 'york',\n",
       " 'wrong',\n",
       " 'think',\n",
       " 'tonight',\n",
       " 'corrupt',\n",
       " 'massive',\n",
       " 'billion',\n",
       " 'important',\n",
       " 'times',\n",
       " 'senator',\n",
       " 'americans',\n",
       " 'called',\n",
       " 'general',\n",
       " 'companies',\n",
       " 'court',\n",
       " 'southern',\n",
       " 'book',\n",
       " 'believe',\n",
       " 'endorsement',\n",
       " 'real',\n",
       " 'true',\n",
       " 'thing',\n",
       " 'market',\n",
       " 'whitehouse',\n",
       " 'women',\n",
       " 'government',\n",
       " 'taxes',\n",
       " 'incredible',\n",
       " 'russian',\n",
       " 'pelosi',\n",
       " 'place',\n",
       " 'open',\n",
       " 'vets',\n",
       " 'presidential',\n",
       " 'watch',\n",
       " 'tremendous',\n",
       " 'political',\n",
       " 'order',\n",
       " 'phony',\n",
       " 'federal',\n",
       " 'happen',\n",
       " 'yesterday',\n",
       " 'healthcare',\n",
       " 'nancy',\n",
       " 'iran',\n",
       " 'pay',\n",
       " 'donald',\n",
       " 'remember',\n",
       " 'having',\n",
       " 'terrible',\n",
       " 'beautiful',\n",
       " 'despite',\n",
       " 'office',\n",
       " 'man',\n",
       " 'future',\n",
       " 'prime',\n",
       " 'minister',\n",
       " 'trying',\n",
       " 'business',\n",
       " 'continue',\n",
       " 'low',\n",
       " 'farmers',\n",
       " 'paid',\n",
       " 'complete',\n",
       " 'truly',\n",
       " 'james',\n",
       " 'start',\n",
       " 'laws',\n",
       " 'schiff',\n",
       " 'obstruction',\n",
       " 'hoax',\n",
       " 'lost',\n",
       " 'enforcement',\n",
       " 'tough',\n",
       " 'protect',\n",
       " 'tomorrow',\n",
       " 'problem',\n",
       " 'god',\n",
       " 'loves',\n",
       " 'highly',\n",
       " 'little',\n",
       " 'wow',\n",
       " 'carolina',\n",
       " 'fight',\n",
       " 'economic',\n",
       " 'badly',\n",
       " 'act',\n",
       " 'stock',\n",
       " 'greatest',\n",
       " 'unemployment',\n",
       " 'major',\n",
       " 'case',\n",
       " 'angry',\n",
       " 'home',\n",
       " 'welcome',\n",
       " 'fantastic',\n",
       " 'secretary',\n",
       " 'wants',\n",
       " 'billions',\n",
       " 'amendment',\n",
       " 'special',\n",
       " 'proud',\n",
       " 'run',\n",
       " 'fighting',\n",
       " 'failing',\n",
       " 'south',\n",
       " 'texas',\n",
       " 'safe',\n",
       " 'family',\n",
       " 'running',\n",
       " 'taking',\n",
       " 'information',\n",
       " 'talk',\n",
       " 'change',\n",
       " 'large',\n",
       " 'leaders',\n",
       " 'families',\n",
       " 'ratings',\n",
       " 'join',\n",
       " 'california',\n",
       " 'safety',\n",
       " 'fast',\n",
       " 'kim',\n",
       " 'allowed',\n",
       " 'supreme',\n",
       " 'isis',\n",
       " 'life',\n",
       " 'radical',\n",
       " 'heading',\n",
       " 'leaving',\n",
       " '2016',\n",
       " 'use',\n",
       " 'hit',\n",
       " 'won',\n",
       " 'investigation',\n",
       " 'level',\n",
       " 'second',\n",
       " 'cut',\n",
       " 'ago',\n",
       " 'given',\n",
       " 'highest',\n",
       " 'impeachment',\n",
       " 'weak',\n",
       " 'stand',\n",
       " 'care',\n",
       " 'number',\n",
       " 'finally',\n",
       " 'congressman',\n",
       " 'statement',\n",
       " 'actually',\n",
       " 'biggest',\n",
       " 'seen',\n",
       " 'close',\n",
       " 'longer',\n",
       " 'morning',\n",
       " 'men',\n",
       " 'ready',\n",
       " 'victory',\n",
       " 'obamacare',\n",
       " 'dishonest',\n",
       " 'system',\n",
       " 'stories',\n",
       " 'took',\n",
       " 'nice',\n",
       " 'based',\n",
       " 'told',\n",
       " 'team',\n",
       " 'workers',\n",
       " 'stated',\n",
       " 'reason',\n",
       " 'gone',\n",
       " 'talking',\n",
       " 'melania',\n",
       " 'crazy',\n",
       " 'intelligence',\n",
       " 'build',\n",
       " 'built',\n",
       " 'daca',\n",
       " 'wo',\n",
       " 'schumer',\n",
       " 'spoke',\n",
       " 'syria',\n",
       " 'votes',\n",
       " 'success',\n",
       " 'possible',\n",
       " 'drugs',\n",
       " 'worse',\n",
       " 'person',\n",
       " 'find',\n",
       " 'read',\n",
       " '3',\n",
       " 'israel',\n",
       " 'whistleblower',\n",
       " 'joe',\n",
       " 'hope',\n",
       " 'dangerous',\n",
       " 'days',\n",
       " 'sad',\n",
       " 'live',\n",
       " 'press',\n",
       " 'lives',\n",
       " 'taken',\n",
       " 'war',\n",
       " 'rally',\n",
       " 'drug',\n",
       " 'energy',\n",
       " 'lowest',\n",
       " 'rate',\n",
       " 'worst',\n",
       " 'week',\n",
       " 'needed',\n",
       " 'gave',\n",
       " 'disaster',\n",
       " 'ohio',\n",
       " 'p.m',\n",
       " 'japan',\n",
       " 'fired',\n",
       " 'zero',\n",
       " '2020',\n",
       " 'thought',\n",
       " 'happening',\n",
       " 'deals',\n",
       " 'dossier',\n",
       " 'approval',\n",
       " 'enjoy',\n",
       " 'service',\n",
       " 'saying',\n",
       " 'friend',\n",
       " 'relationship',\n",
       " 'false',\n",
       " 'bring',\n",
       " 'reporting',\n",
       " 'unfair',\n",
       " 'away',\n",
       " 'guy',\n",
       " 'ridiculous',\n",
       " 'fair',\n",
       " 'fed',\n",
       " 'fix',\n",
       " 'd.c',\n",
       " 'rigged',\n",
       " 'w',\n",
       " 'respected',\n",
       " 'millions',\n",
       " 'winning',\n",
       " 'nations',\n",
       " 'asked',\n",
       " 'pennsylvania',\n",
       " 'reform',\n",
       " 'bob',\n",
       " 'words',\n",
       " 'post',\n",
       " 'department',\n",
       " 'successful',\n",
       " 'amazing',\n",
       " 'power',\n",
       " 'decision',\n",
       " 'poll',\n",
       " 'immediately',\n",
       " 'meetings',\n",
       " 'turkey',\n",
       " 'adam']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see how many words we need to build 50% of the tweet corpus\n",
    "total = 0\n",
    "words_50 = []\n",
    "for word, count in word_freq.most_common():\n",
    "    total += count\n",
    "    words_50.append(word)\n",
    "    if total/sum(word_freq.values()) > 0.5:\n",
    "        break\n",
    "words_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 401 words covers 50% of the entire corpus\n",
    "len(words_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('esposito', 1),\n",
       " ('consulting', 1),\n",
       " ('donaldjtrumpjr', 1),\n",
       " ('triggered', 1),\n",
       " ('thrives', 1)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at the least frequent words\n",
    "word_freq.most_common()[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('our country', 540), ('the democrats', 360), ('china', 358), ('people', 308), ('the united states', 285)]\n"
     ]
    }
   ],
   "source": [
    "# most frequent noun chunks \n",
    "noun_chunks = [chunk.text.lower() for chunk in doc.noun_chunks\n",
    "              if chunk.text.lower() not in stopwords\n",
    "              and chunk.text not in misc]\n",
    "noun_chunk_freq = Counter(noun_chunks)\n",
    "common_noun_chunks = noun_chunk_freq.most_common(5)\n",
    "print(common_noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('president', 835), ('democrats', 739), ('trump', 645), ('fake', 588), ('news', 579)]\n"
     ]
    }
   ],
   "source": [
    "proper_nouns = [word.text.lower() for word in doc if word.pos_ == 'PROPN'\n",
    "               and word.text not in misc]\n",
    "proper_noun_freq = Counter(proper_nouns)\n",
    "common_proper_nouns = proper_noun_freq.most_common(5)\n",
    "print(common_proper_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('make', 881), ('do', 789), ('go', 764), ('thank', 704), ('want', 633)]\n"
     ]
    }
   ],
   "source": [
    "verbs = [word.lemma_ for word in doc if word.pos_=='VERB'\n",
    "       and word.text not in misc]\n",
    "verb_freq = Counter(verbs)\n",
    "common_verbs = verb_freq.most_common(5)\n",
    "print(common_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('trump', 210), ('witch hunt', 127), ('hillary clinton', 91), ('hillary', 71), ('obama', 66)]\n"
     ]
    }
   ],
   "source": [
    "# these values seem low... Entity tagging may not be working properly\n",
    "ents = [ent.text.lower().strip().strip(punct) for ent in doc.ents if ent.label_=='PERSON']\n",
    "ent_freq = Counter(ents)\n",
    "common_ents = ent_freq.most_common(5)\n",
    "print(common_ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's just see how this stacks up to the whole document without spacy\n",
    "text = open('../data/tweets/tweet_text.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136414"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# considerably fewere words due to no tokenization here\n",
    "all_words = [word.lower().strip().strip(punct) for word in text.split() \n",
    "             if word.lower() not in stopwords \n",
    "             and word.lower() not in punct\n",
    "            and word.lower() not in misc]\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 2133),\n",
       " ('people', 1065),\n",
       " ('president', 802),\n",
       " ('country', 763),\n",
       " ('news', 753)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# good to see that we get the same results here as above (see word_freq)\n",
    "freq = Counter(all_words)\n",
    "freq.most_common(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
