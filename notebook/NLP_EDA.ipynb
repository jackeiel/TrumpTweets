{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create one big Doc object, pickle it for later use\n",
    "tweets = open('../data/tweets/tweet_text.txt', 'r').read()\n",
    "nlp.max_length = 2000000\n",
    "doc = nlp(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can comment out with statement after the pkl is written\n",
    "with open('../data/tweets/tweets_nlp_doc.pkl', 'wb') as file:\n",
    "    pickle.dump(doc, file)\n",
    "\n",
    "doc = pickle.load(open('../data/tweets/tweets_nlp_doc.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360422"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('great', 1367), ('President', 1188), ('people', 1019), ('Trump', 1006), ('Democrats', 866)]\n",
      "\n",
      "\n",
      "[('people', 1019), ('time', 548), ('years', 522), ('today', 408), ('job', 371)]\n"
     ]
    }
   ],
   "source": [
    "# all tokens that arent stop words or punctuations\n",
    "misc = ['\\n', '\\n  ', 'amp', ' ']\n",
    "\n",
    "words = [token.text for token in doc if token.is_stop != True \n",
    "         and token.is_punct != True \n",
    "         and token.text not in misc]\n",
    "\n",
    "# noun tokens that arent stop words or punctuations\n",
    "nouns = [token.text for token in doc if token.is_stop != True \n",
    "         and token.is_punct != True \n",
    "         and token.pos_ == \"NOUN\"]\n",
    "\n",
    "# five most common tokens\n",
    "word_freq = Counter(words)\n",
    "common_words = word_freq.most_common(5)\n",
    "print(common_words)\n",
    "print('\\n')\n",
    "\n",
    "# five most common noun tokens\n",
    "noun_freq = Counter(nouns)\n",
    "common_nouns = noun_freq.most_common(5)\n",
    "print(common_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 2739), ('it', 1626), ('you', 1568), ('they', 1335), ('amp', 1263)]\n"
     ]
    }
   ],
   "source": [
    "# this doesn't look like it's working properly... should be getting longer chunks\n",
    "noun_chunks = [chunk.text for chunk in doc.noun_chunks]\n",
    "noun_chunk_freq = Counter(noun_chunks)\n",
    "common_noun_chunks = noun_chunk_freq.most_common(5)\n",
    "print(common_noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('President', 1188), ('Trump', 956), ('Democrats', 857), ('U.S.', 625), ('News', 574)]\n"
     ]
    }
   ],
   "source": [
    "proper_nouns = [word.text for word in doc if word.pos_ == 'PROPN'\n",
    "               and word.text not in misc]\n",
    "proper_noun_freq = Counter(proper_nouns)\n",
    "common_proper_nouns = proper_noun_freq.most_common(5)\n",
    "print(common_proper_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('make', 986), ('do', 858), ('go', 840), ('thank', 788), ('want', 684)]\n"
     ]
    }
   ],
   "source": [
    "verbs = [word.lemma_ for word in doc if word.pos_=='VERB'\n",
    "       and word.text not in misc]\n",
    "verb_freq = Counter(verbs)\n",
    "common_verbs = verb_freq.most_common(5)\n",
    "print(common_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(God, 1), (ðŸ‡º, 1), (joy love, 1), (Bill Clinton, 1), (America \n",
      ", 1)]\n"
     ]
    }
   ],
   "source": [
    "# this also doesn't appear to work very well on the large text doc\n",
    "ents = [ent for ent in doc.ents if ent.label_=='PERSON']\n",
    "ent_freq = Counter(ents)\n",
    "common_ents = ent_freq.most_common(5)\n",
    "print(common_ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That U thing represents the first part of an American flag emoji, will have to figure out how to remove it later on"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
